{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N7dTqu2gXWjt",
    "outputId": "62453676-ffe3-4535-abe0-f3b85de81a5d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/shubham.das/.local/lib/python3.5/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yePog9vXcPL"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "total_words = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8EsUclkXiMZ"
   },
   "outputs": [],
   "source": [
    "def dataset_preparation(data):\n",
    "\n",
    "\t# basic cleanup\n",
    "\tcorpus = data.lower().split(\"\\n\")\n",
    "\n",
    "\t# tokenization\t\n",
    "\ttokenizer.fit_on_texts(corpus)\n",
    "\ttotal_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "\t# create input sequences using list of tokens\n",
    "\tinput_sequences = []\n",
    "\tfor line in corpus:\n",
    "\t\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\t\tfor i in range(1, len(token_list)):\n",
    "\t\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "\t# pad sequences \n",
    "\tmax_sequence_len = max([len(x) for x in input_sequences])\n",
    "\tprint(max_sequence_len)\n",
    "\tinput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\t# create predictors and label\n",
    "\tpredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "# \tlabel = ku.to_categorical(label, num_classes=total_words)\n",
    "\n",
    "\treturn predictors, label, max_sequence_len, total_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrTDaZ68Xm3Z"
   },
   "outputs": [],
   "source": [
    "def create_model(predictors, label, max_sequence_len, total_words):\n",
    "\t\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
    "    model.add(LSTM(150, return_sequences = True))\n",
    "#     model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "#     earlystop = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "#     model.fit_generator(generator(predictors, label, BATCH_SIZE) , steps_per_epoch=int(len(predictors)/BATCH_SIZE) + 1, epochs=50, verbose=1, callbacks=[earlystop])\n",
    "    return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mP3GIUzuZqZI"
   },
   "outputs": [],
   "source": [
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(batch_size):\n",
    "            x.append(sentence_list[index])\n",
    "            y.append(ku.to_categorical(next_word_list[index], num_classes=total_words))\n",
    "            index = index+1\n",
    "            if index == len(sentence_list):\n",
    "                index = 0\n",
    "        yield np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIIlR8GSXsSa"
   },
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len,model):\n",
    "\tfor _ in range(next_words):\n",
    "\t\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\t\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\t\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\t\t\n",
    "\t\toutput_word = \"\"\n",
    "\t\tfor word, index in tokenizer.word_index.items():\n",
    "\t\t\tif index == predicted:\n",
    "\t\t\t\toutput_word = word\n",
    "\t\t\t\tbreak\n",
    "\t\tseed_text += \" \" + output_word\n",
    "\treturn seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zYwedSXzX61p"
   },
   "outputs": [],
   "source": [
    "data = open('en_US_3.txt', encoding=\"utf8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuWohX4w9DWs"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jQEvFtvfYRs2",
    "outputId": "d6613cab-c097-4f53-a6e3-a21257c47e76",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "(697219, 38) (697219,)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 38, 10)            442800    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 38, 150)           96600     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               100400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 44280)             4472280   \n",
      "=================================================================\n",
      "Total params: 5,112,080\n",
      "Trainable params: 5,112,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "10895/10895 [==============================] - 1728s 159ms/step - loss: 5.0398 - accuracy: 0.1564\n",
      "Epoch 2/50\n",
      "10895/10895 [==============================] - 1734s 159ms/step - loss: 4.9521 - accuracy: 0.1647\n",
      "Epoch 3/50\n",
      "10895/10895 [==============================] - 1718s 158ms/step - loss: 4.9243 - accuracy: 0.1676\n",
      "Epoch 4/50\n",
      "10895/10895 [==============================] - 1730s 159ms/step - loss: 4.9086 - accuracy: 0.1695\n",
      "Epoch 5/50\n",
      "10895/10895 [==============================] - 1736s 159ms/step - loss: 4.8952 - accuracy: 0.1710\n",
      "Epoch 6/50\n",
      "10895/10895 [==============================] - 1733s 159ms/step - loss: 4.8849 - accuracy: 0.1719\n",
      "Epoch 7/50\n",
      "10895/10895 [==============================] - 1716s 157ms/step - loss: 4.8733 - accuracy: 0.1732\n",
      "Epoch 8/50\n",
      "10895/10895 [==============================] - 1738s 159ms/step - loss: 4.8632 - accuracy: 0.1743\n",
      "Epoch 9/50\n",
      "10895/10895 [==============================] - 1737s 159ms/step - loss: 4.8540 - accuracy: 0.1757\n",
      "Epoch 10/50\n",
      "10895/10895 [==============================] - 1737s 159ms/step - loss: 4.8463 - accuracy: 0.1766\n",
      "Epoch 11/50\n",
      "10895/10895 [==============================] - 1719s 158ms/step - loss: 4.8365 - accuracy: 0.1773\n",
      "Epoch 12/50\n",
      "10895/10895 [==============================] - 1735s 159ms/step - loss: 4.8297 - accuracy: 0.1781\n",
      "Epoch 13/50\n",
      "10895/10895 [==============================] - 1721s 158ms/step - loss: 4.8209 - accuracy: 0.1790\n",
      "Epoch 14/50\n",
      "10895/10895 [==============================] - 1740s 160ms/step - loss: 4.8142 - accuracy: 0.1797\n",
      "Epoch 15/50\n",
      "10895/10895 [==============================] - 1745s 160ms/step - loss: 4.8054 - accuracy: 0.1806\n",
      "Epoch 16/50\n",
      "10895/10895 [==============================] - 1747s 160ms/step - loss: 4.8002 - accuracy: 0.1812\n",
      "Epoch 17/50\n",
      "10895/10895 [==============================] - 1743s 160ms/step - loss: 4.7916 - accuracy: 0.1820\n",
      "Epoch 18/50\n",
      "10895/10895 [==============================] - 1746s 160ms/step - loss: 4.7858 - accuracy: 0.1829\n",
      "Epoch 19/50\n",
      "10895/10895 [==============================] - 1771s 163ms/step - loss: 4.7794 - accuracy: 0.1837\n",
      "Epoch 20/50\n",
      "10895/10895 [==============================] - 1841s 169ms/step - loss: 4.7729 - accuracy: 0.1841\n",
      "Epoch 21/50\n",
      "10895/10895 [==============================] - 1791s 164ms/step - loss: 4.7656 - accuracy: 0.1852\n",
      "Epoch 22/50\n",
      "10895/10895 [==============================] - 1744s 160ms/step - loss: 4.7604 - accuracy: 0.1859\n",
      "Epoch 23/50\n",
      "10895/10895 [==============================] - 1728s 159ms/step - loss: 4.7556 - accuracy: 0.1861\n",
      "Epoch 24/50\n",
      "10895/10895 [==============================] - 1720s 158ms/step - loss: 4.7489 - accuracy: 0.1872\n",
      "Epoch 25/50\n",
      "10895/10895 [==============================] - 1732s 159ms/step - loss: 4.7432 - accuracy: 0.1874\n",
      "Epoch 26/50\n",
      "10895/10895 [==============================] - 1731s 159ms/step - loss: 4.7379 - accuracy: 0.1882\n",
      "Epoch 27/50\n",
      "10895/10895 [==============================] - 1734s 159ms/step - loss: 4.7325 - accuracy: 0.1889\n",
      "Epoch 28/50\n",
      "10895/10895 [==============================] - 1733s 159ms/step - loss: 4.7274 - accuracy: 0.1894\n",
      "Epoch 29/50\n",
      "10895/10895 [==============================] - 1713s 157ms/step - loss: 4.7216 - accuracy: 0.1901\n",
      "Epoch 30/50\n",
      "10895/10895 [==============================] - 1731s 159ms/step - loss: 4.7179 - accuracy: 0.1906\n",
      "Epoch 31/50\n",
      "10895/10895 [==============================] - 1736s 159ms/step - loss: 4.7141 - accuracy: 0.1908\n",
      "Epoch 32/50\n",
      "10895/10895 [==============================] - 1736s 159ms/step - loss: 4.7068 - accuracy: 0.1916\n",
      "Epoch 33/50\n",
      "10895/10895 [==============================] - 1736s 159ms/step - loss: 4.7032 - accuracy: 0.1923\n",
      "Epoch 34/50\n",
      "10895/10895 [==============================] - 1746s 160ms/step - loss: 4.6987 - accuracy: 0.1929\n",
      "Epoch 35/50\n",
      "10895/10895 [==============================] - 1737s 159ms/step - loss: 4.6937 - accuracy: 0.1933\n",
      "Epoch 36/50\n",
      "10895/10895 [==============================] - 1784s 164ms/step - loss: 4.6896 - accuracy: 0.1936\n",
      "Epoch 37/50\n",
      "10895/10895 [==============================] - 1755s 161ms/step - loss: 4.6874 - accuracy: 0.1940\n",
      "Epoch 38/50\n",
      "10895/10895 [==============================] - 1746s 160ms/step - loss: 4.6825 - accuracy: 0.1943\n",
      "Epoch 39/50\n",
      "10895/10895 [==============================] - 1746s 160ms/step - loss: 4.6772 - accuracy: 0.1953\n",
      "Epoch 40/50\n",
      "10895/10895 [==============================] - 1749s 161ms/step - loss: 4.6739 - accuracy: 0.1957\n",
      "Epoch 41/50\n",
      "10895/10895 [==============================] - 1746s 160ms/step - loss: 4.6685 - accuracy: 0.1962\n",
      "Epoch 42/50\n",
      "10895/10895 [==============================] - 1739s 160ms/step - loss: 4.6649 - accuracy: 0.1966\n",
      "Epoch 43/50\n",
      "10895/10895 [==============================] - 1746s 160ms/step - loss: 4.6603 - accuracy: 0.1966\n",
      "Epoch 44/50\n",
      "10895/10895 [==============================] - 1829s 168ms/step - loss: 4.6556 - accuracy: 0.1971\n",
      "Epoch 45/50\n",
      "10895/10895 [==============================] - 1842s 169ms/step - loss: 4.6527 - accuracy: 0.1978\n",
      "Epoch 46/50\n",
      "10895/10895 [==============================] - 1834s 168ms/step - loss: 4.6479 - accuracy: 0.1983\n",
      "Epoch 47/50\n",
      "10895/10895 [==============================] - 1885s 173ms/step - loss: 4.6466 - accuracy: 0.1986\n",
      "Epoch 48/50\n",
      "10895/10895 [==============================] - 1836s 168ms/step - loss: 4.6403 - accuracy: 0.1990\n",
      "Epoch 49/50\n",
      "10895/10895 [==============================] - 1833s 168ms/step - loss: 4.6388 - accuracy: 0.1990\n",
      "Epoch 50/50\n",
      "10895/10895 [==============================] - 1810s 166ms/step - loss: 4.6309 - accuracy: 0.2002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x14cb6a5624a8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors, label, max_sequence_len, total_words = dataset_preparation(data)\n",
    "print(predictors.shape, label.shape)\n",
    "model = create_model(predictors, label, max_sequence_len, total_words)\n",
    "model.load_weights('model_weights.h5')\n",
    "earlystop = EarlyStopping(monitor='loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "model.fit_generator(generator(predictors, label, BATCH_SIZE) , steps_per_epoch=int(len(predictors)/BATCH_SIZE) + 1, epochs=50, verbose=1, callbacks=[earlystop])\n",
    "\n",
    "# print (generate_text(\"we naughty\", 3, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "3shs4lDr6-aM",
    "outputId": "ff4802b1-3ece-4ebc-d88b-a2d55860815b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh man i hate cheesecake i don't know what i don't know what i was a dancer and i have a 300 store and i have a crush for adding in the\n"
     ]
    }
   ],
   "source": [
    "print (generate_text(\"oh man\", 30, max_sequence_len,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CmrDB09XR-c4"
   },
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
