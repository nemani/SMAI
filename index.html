<!DOCTYPE html>
<!-- saved from url=(0018)https://vibora.io/ -->
<html lang="en" class="fontawesome-i2svg-active fontawesome-i2svg-complete"><head>
    <meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=yes">
    <title> SMAI | Social Media API for IRE</title>
    
    <link rel="stylesheet" href="./assets/all.css">
    <script src="./assets/jquery-3.2.1.min.js"></script>
    <script src="./assets/bootstrap.bundle.min.js"></script>
    <script src="./assets/highlight.js"></script>
    <script defer="" src="./assets/fontawesome-all.js"></script>
</head>
<body>

<div class="container mt-100">
    <img src="./assets/SMAI.png" width="200">
</div>

<div class="container">
    <header>
        <div class="row">
            <div class="hand-written" size="50"><br></div><div class="hand-written" size="50"><br></div><div class="hand-written" size="50"><br></div><div class="hand-written" size="50">S</div><div class="hand-written" size="50"><br></div><div class="hand-written" size="50"><br></div><div class="hand-written" size="50">MAI Media API for Ire project</div><div class="hand-written" size="50"><br></div><div class="hand-written" size="50"><br></div><div class="hand-written" size="50"><br></div>
            <div class="col-lg-10 col-sm-12">
                <ul class="pull-right-lg"><br></ul>
            </div>
        </div>
    </header>
</div>

    <div class="container">
        <div class="row">
            <div class="col-lg-6 order-2 order-sm-2"><font color="#ffe567" face="Architects Daughter, cursive"><span style="font-size: 36px;">LANGUAGE MODEL</span></font><br>
                <div class="description">Language Modeling and LM for short is the development of probabilistic models that are able to predict the next word in the sequence given the words that precede it.</div><div class="description"><br>
                    
                        <button class="btn" id="btn-get-started"><a href="https://docs.vibora.io/"><svg class="svg-inline--fa fa-play-circle fa-w-16" aria-hidden="true" data-fa-processed="" data-prefix="far" data-icon="play-circle" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M371.7 238l-176-107c-15.8-8.8-35.7 2.5-35.7 21v208c0 18.4 19.8 29.8 35.7 21l176-101c16.4-9.1 16.4-32.8 0-42zM504 256C504 119 393 8 256 8S8 119 8 256s111 248 248 248 248-111 248-248zm-448 0c0-110.5 89.5-200 200-200s200 89.5 200 200-89.5 200-200 200S56 366.5 56 256z"></path></svg><!-- <i class="far fa-play-circle"></i> -->&nbsp;</a>Demo</button></div>
            </div>
            <div class="col-lg-6 order-lg-1 d-none d-sm-block"><div class="chartjs-size-monitor" style="position: absolute; left: 0px; top: 0px; right: 0px; bottom: 0px; overflow: hidden; pointer-events: none; visibility: hidden; z-index: -1;"><div class="chartjs-size-monitor-expand" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;pointer-events:none;visibility:hidden;z-index:-1;"><div style="position:absolute;width:1000000px;height:1000000px;left:0;top:0"></div></div><div class="chartjs-size-monitor-shrink" style="position:absolute;left:0;top:0;right:0;bottom:0;overflow:hidden;pointer-events:none;visibility:hidden;z-index:-1;"><div style="position:absolute;width:200%;height:200%;left:0; top:0"></div></div></div>
                <canvas id="canvasMain" style="width: 690px; display: block; height: 345px;" width="690" height="345" class="chartjs-render-monitor"></canvas>
                <script>
                    var barChartData = {
                        labels: ["Django 1.10", "Flask", "Pyramid", "Apistar", "Weppy", "Sanic", "Vibora"],
                        data: [{x: 'Vibora', y: 20}],
                        datasets: [{
                            label: 'Requests per second',
                            borderWidth: 1,
                            data: [30000, 33000, 35000, 36000, 45000, 60000, 150000],
                            backgroundColor: [
                                '#e8aeae',
                                '#e8aeae',
                                '#e8aeae',
                                '#e88e96',
                                '#e87386',
                                '#e85b77',
                                '#E83561'
                            ]
                        }]
                    };
                    var ctx = document.getElementById("canvasMain").getContext("2d");
                    window.myBar = new Chart(ctx, {
                        type: 'bar',
                        data: barChartData,
                        options: {
                            responsive: true,
                            legend: {
                                display: false,
                                position: 'top',
                            },
                            title: {
                                display: false,
                                text: ''
                            },
                            scales: {
                                yAxes: [{
                                    ticks: {
                                        // Include a dollar sign in the ticks
                                        callback: function (value, index, values) {
                                            return (value / 1000) + "k";
                                        }
                                    }
                                }]
                            }
                        }
                    });
                </script>
            </div>
        </div>
        <hr>
        <!-- Sexy and Efficient -->
        <div class="row">
            <div class="col-lg-7 topic"><font color="#ffe567" face="Architects Daughter, cursive"><span style="font-size: 36px;">N-GRAM MODEL</span></font><br><br>
                <div class="description">It is a model that assigns probabilities LM to sentences and sequences of words, the n-gram. An n-gram is a sequence of N n-gram words: a 2-gram (or bigram) is a two-word sequence of words like �please turn�, �turn your�, or �your homework�, and a 3-gram (or trigram) is a three-word sequence of words like �please turn your�, or �turn your homework�.</div><div class="description"><br></div><div class="description"><br></div></div><div class="col-lg-5"><pre>                <code class="python hljs">
<br></code>
            </pre>
            </div>
        </div>
        
<div class="row">
            <div class="col-lg-6 order-2" style="padding-top:120px;">
            <img src="./assets/lstm5.png" width="500" height="205" style="background-color: transparent; color: rgb(0, 0, 0); font-family: Arial; font-size: 11pt; white-space: pre-wrap; margin-left: 0px; margin-top: 0px;">
            </div>
            <div class="col-lg-6 topic order-sm-1 order-lg-2"><font color="#ffe567" face="Architects Daughter, cursive"><span style="font-size: 36px;">LSTM</span></font><br>
                <div class="description">LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!<br><br></div><div class="description"><div class="description">LSTMs also have this chain-like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.</div><div class="description"><br></div></div></div></div><div style="text-align:center">
<font color="#ffe567" face="Architects Daughter, cursive" align="center"><span style="font-size: 36px;">Methodologies tried</span></font>
</div>
<div class="row">

<div class="col-lg-6 topic order-sm-1 order-lg-2"><span style="color: rgb(255, 229, 103); font-family: &quot;Architects Daughter&quot;, cursive; font-size: 36px; text-align: center;"><u>NGram</u></span><br>
                <div class="description"><div class="description">In-Text Generation using Language Modeling, we can simply follow the greedy approach of picking the word with the highest probability trained based on the training corpus. This method can give good local results but the quality of the entire sentence as a whole is very poor. In order to serve this better another method �beam search� was proposed.</div><div class="description">The local beam search algorithm keeps track of k states rather than just one. It begins with k randomly generated states. At each step, all the successors of all k states are generated. If anyone is a goal(depending upon the implementation), the algorithm halts. Otherwise, it selects the k best successors from the complete list and repeats.</div><div class="description">To this we added add-1 smoothing and back-off method also.</div><div class="description">This was used in n-gram model implementation.</div><div><br></div><div><br></div></div><div class="description"><div class="description"><br></div></div></div><div class="col-lg-6 topic order-sm-1 order-lg-2"><u><span style="color: rgb(255, 229, 103); font-family: &quot;Architects Daughter&quot;, cursive; font-size: 36px; text-align: center;">LSTM</span><br>
                </u><div class="description"><div class="description"><div class="description">A word embedding is a class of approaches for representing words and documents using a dense vector representation.</div><div class="description">It is an improvement over the traditional bag-of-word model encoding schemes where large sparse vectors were used to represent each word or to score each word within a vector to represent an entire vocabulary. These representations were sparse because the vocabularies were vast and a given word or document would be represented by a large vector comprised mostly of zero values.</div><div class="description">Instead, in an embedding, words are represented by dense vectors where a vector represents the projection of the word into a continuous vector space.</div></div></div></div></div></div><div style="text-align: center;"><font color="#ffe567" face="Architects Daughter, cursive" align="center"><span style="font-size: 36px;">Architecture</span></font></div><div class="row">
            <div class="col-lg-6 order-2" style="padding-top:120px;">
            <img src="./assets/lstm.png" width="624" height="57" style="background-color: transparent; color: rgb(0, 0, 0); font-family: Arial; font-size: 11pt; font-weight: 700; white-space: pre-wrap; text-align: left; margin-left: 0px; margin-top: 0px;"></div><div class="col-lg-6 topic order-sm-1 order-lg-2">
                <div class="description"><div class="description">Here the Embedding and dense layers are fixed number of LSTM layers and numbers of cells in each layer are given as input by the user to the API.</div><div class="description">In the embedding layer output_dimension is kept at 10 bit vector per word. In the Dense layer, the activation function is fixed at softmax for getting probabilities for each word.</div><div class="description">Optimizer for the model is fixed as Adam optimizer and although metric is kept as the accuracy and the loss considered as categorical_crossentropy.</div><div><br></div></div></div></div><div class="row"></div><br>
    <script>hljs.initHighlightingOnLoad();</script>

<div class="clearfix"></div>
<footer style="background: #1c1e26; border-top: 3px dashed #2C2F38">
    <div class="container">
        <div class="row">
            <div class="col-lg-6">
                <svg class="svg-inline--fa fa-heartbeat fa-w-18" aria-hidden="true" data-fa-processed="" data-prefix="fa" data-icon="heartbeat" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path fill="currentColor" d="M47.9 257C31.6 232.7 16 200.5 16 165.5 16 76.9 70.3 24 161.1 24 214.2 24 264 65.7 288 89.3 312 65.7 361.8 24 414.9 24 505.7 24 560 76.9 560 165.5c0 35-15.5 67.2-31.9 91.5H408l-26.4-58.6c-4.7-8.9-17.6-8.5-21.6.7l-53.3 134.6L235.4 120c-3.7-10.6-18.7-10.7-22.6-.2l-48 137.2H47.9zm348 32c-4.5 0-8.6-2.5-10.6-6.4l-12.8-32.5-56.9 142.8c-4.4 9.9-18.7 9.4-22.3-.9l-69.7-209.2-33.6 98.4c-1.7 4.7-6.2 7.8-11.2 7.8H73.4c5.3 5.7-12.8-12 198.9 192.6 8.8 8.5 22.8 8.5 31.6 0 204.3-197.2 191-184 199-192.6h-107z"></path></svg><!-- <i class="fa fa-heartbeat"></i> -->Made with love by <a href="https://github.com/frnkvieira" target="_blank">Frank Vieira</a>
            </div>
            <div class="col-lg-6" style="text-align: right;">
                Give Vibora a star on
                <a href="https://github.com/vibora-io/vibora" style="margin-left: 3px;">
                    <svg class="svg-inline--fa fa-github fa-w-16" style="margin-right: 3px;" aria-hidden="true" data-fa-processed="" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github" style="margin-right: 3px;"></i> -->Github!
                </a>
            </div>
        </div>
    </div>
</footer>

</body></html>